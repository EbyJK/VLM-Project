{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "# This may fail on some runtimes but try:\n",
        "!nvidia-smi || true\n",
        "\n"
      ],
      "metadata": {
        "id": "vsg6FAuB1rUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wSgLgdIzhcl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torchvision timm accelerate Pillow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# now you can read/write at /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "8LLWg1vv4ZOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Test images.zip\" -d .\n",
        "!ls -la test_images | sed -n '1,40p'\n"
      ],
      "metadata": {
        "id": "zp_JusYm4gdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8lvav6VL65jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R test_images\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eBKgKbJP57Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uJGWVTZ9UAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1uj7brFr6-E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch, os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "test_folder = \"test_images/Test images\"\n",
        "\n",
        "for fname in sorted(os.listdir(test_folder)):\n",
        "    path = os.path.join(test_folder, fname)\n",
        "    if os.path.isfile(path):  # ‚úÖ Only process files, skip folders\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        out = model.generate(**inputs, max_new_tokens=50)\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"\\n{fname} -> {caption}\")\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Vjsge6DV5Mj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch, glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# Recursively find all images\n",
        "image_paths = glob.glob(\"test_images/Test images/**/*.*\", recursive=True)\n",
        "print(f\"Found {len(image_paths)} images\")\n",
        "\n",
        "# Loop through first 10 images\n",
        "for path in image_paths[:10]:\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(images=image,text=\"A photo of\", return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    repetition_penalty=2.0,\n",
        "    num_beams=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\nüì∑ {path} -> {caption}\")\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rft7egDs7RmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch, glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- CATEGORY MAPPING ----------\n",
        "CATEGORY_KEYWORDS = {\n",
        "    \"Fruit\": [\"apple\", \"banana\", \"mango\", \"orange\", \"grape\", \"tomato\", \"pineapple\", \"papaya\", \"watermelon\", \"strawberry\", \"citrus\"],\n",
        "    \"Vegetable\": [\"carrot\", \"onion\", \"potato\", \"broccoli\", \"lettuce\", \"spinach\", \"cauliflower\", \"beans\", \"peas\"],\n",
        "    \"Spice\": [\"mustard\", \"cardamom\", \"cinnamon\", \"clove\", \"nutmeg\", \"fennel\", \"fenugreek\", \"bay leaf\", \"coriander\", \"turmeric\", \"pepper\", \"chilli\"],\n",
        "    \"Dry Fruit\": [\"almond\", \"cashew\", \"raisin\", \"pistachio\", \"walnut\", \"hazelnut\", \"dates\", \"fig\", \"apricot\"],\n",
        "    \"Pulse\": [\"lentil\", \"gram\", \"bean\", \"dhal\", \"dal\", \"pea\"]\n",
        "}\n",
        "\n",
        "def classify_caption(caption):\n",
        "    caption_lower = caption.lower()\n",
        "    for category, keywords in CATEGORY_KEYWORDS.items():\n",
        "        if any(keyword in caption_lower for keyword in keywords):\n",
        "            return category\n",
        "    return \"Unknown\"\n",
        "\n",
        "# ---------- SETUP MODEL ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# ---------- LOAD IMAGES ----------\n",
        "image_paths = glob.glob(\"test_images/Test images/**/*.*\", recursive=True)\n",
        "print(f\"Found {len(image_paths)} images\")\n",
        "\n",
        "# ---------- PROCESS & LOG ----------\n",
        "with open(\"vlm_results.txt\", \"w\") as log_file:\n",
        "    for path in image_paths:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Prompt-based captioning to avoid loops\n",
        "        inputs = processor(images=image, text=\"A photo of\", return_tensors=\"pt\").to(device)\n",
        "        out = model.generate(**inputs, max_new_tokens=50, repetition_penalty=2.0, num_beams=5, early_stopping=True)\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        # Classify\n",
        "        category = classify_caption(caption)\n",
        "\n",
        "        # Print & show\n",
        "        print(f\"\\nüì∑ {path}\")\n",
        "        print(f\"üß† Caption: {caption}\")\n",
        "        print(f\"üè∑Ô∏è Category: {category}\")\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{category}: {caption}\")\n",
        "        plt.show()\n",
        "\n",
        "        # Log to file\n",
        "        log_file.write(f\"{path} | {caption} | {category}\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SXcy1oNI9urz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoProcessor, AutoModelForCausalLM\n",
        "import torch, glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- CATEGORY MAPPING with plurals & synonyms ----------\n",
        "CATEGORY_KEYWORDS = {\n",
        "    \"Fruit\": [\"apple\", \"apples\", \"banana\", \"bananas\", \"mango\", \"mangos\", \"mangoes\", \"orange\", \"oranges\",\n",
        "              \"grape\", \"grapes\", \"tomato\", \"tomatoes\", \"pineapple\", \"papaya\", \"watermelon\", \"strawberry\", \"strawberries\", \"citrus\", \"lemon\", \"lemons\", \"lime\", \"limes\"],\n",
        "    \"Vegetable\": [\"carrot\", \"carrots\", \"onion\", \"onions\", \"potato\", \"potatoes\", \"broccoli\", \"lettuce\", \"spinach\", \"cauliflower\", \"beans\", \"bean\", \"peas\", \"pea\"],\n",
        "    \"Spice\": [\"mustard\", \"cardamom\", \"cardamoa\", \"cinnamon\", \"clove\", \"nutmeg\", \"fennel\", \"fenugreek\", \"bay leaf\", \"bay leaves\", \"coriander\", \"turmeric\", \"pepper\", \"chilli\", \"chillies\"],\n",
        "    \"Dry Fruit\": [\"almond\", \"almonds\", \"cashew\", \"cashews\", \"raisin\", \"raisins\", \"pistachio\", \"pistachios\", \"walnut\", \"walnuts\", \"hazelnut\", \"hazelnuts\", \"dates\", \"date\", \"fig\", \"figs\", \"apricot\", \"apricots\"],\n",
        "    \"Pulse\": [\"lentil\", \"lentils\", \"gram\", \"grams\", \"bean\", \"beans\", \"dhal\", \"dal\", \"pea\", \"peas\"]\n",
        "}\n",
        "\n",
        "def classify_caption(caption):\n",
        "    caption_lower = caption.lower()\n",
        "    for category, keywords in CATEGORY_KEYWORDS.items():\n",
        "        if any(keyword in caption_lower for keyword in keywords):\n",
        "            return category\n",
        "    return \"Unknown\"\n",
        "\n",
        "# ---------- Setup Models ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# BLIP\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# GIT-VLM (fallback)\n",
        "git_model_name = \"microsoft/git-base\"\n",
        "git_processor = AutoProcessor.from_pretrained(git_model_name)\n",
        "git_model = AutoModelForCausalLM.from_pretrained(git_model_name).to(device)\n",
        "\n",
        "# ---------- Caption Functions ----------\n",
        "def blip_caption(image, prompt=\"A photo of\"):\n",
        "    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    out = blip_model.generate(**inputs, max_new_tokens=50, repetition_penalty=2.0, num_beams=5, early_stopping=True)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def git_caption(image):\n",
        "    pixel_values = git_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    generated_ids = git_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    return git_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "def is_repetitive(caption, threshold=3):\n",
        "    words = caption.lower().split()\n",
        "    return any(words.count(w) >= threshold for w in set(words))\n",
        "\n",
        "# ---------- Load Images ----------\n",
        "image_paths = glob.glob(\"test_images/Test images/**/*.*\", recursive=True)\n",
        "print(f\"Found {len(image_paths)} images\")\n",
        "\n",
        "# ---------- Process & Log ----------\n",
        "with open(\"vlm_results_v2.txt\", \"w\") as log_file:\n",
        "    for path in image_paths:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # Step 1: Try BLIP\n",
        "        caption = blip_caption(image)\n",
        "\n",
        "        # Step 2: Retry with stronger prompt if repetitive\n",
        "        if is_repetitive(caption):\n",
        "            caption = blip_caption(image, prompt=\"Describe the main object in this image\")\n",
        "\n",
        "        # Step 3: If still repetitive or too short, use GIT-VLM\n",
        "        if is_repetitive(caption) or len(caption.split()) < 3:\n",
        "            caption = git_caption(image)\n",
        "\n",
        "        # Step 4: Classify\n",
        "        category = classify_caption(caption)\n",
        "\n",
        "        # Step 5: Show & Log\n",
        "        print(f\"\\nüì∑ {path}\")\n",
        "        print(f\"üß† Caption: {caption}\")\n",
        "        print(f\"üè∑Ô∏è Category: {category}\")\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{category}: {caption}\")\n",
        "        plt.show()\n",
        "\n",
        "        log_file.write(f\"{path} | {caption} | {category}\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "stSN7Bk8_B81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gradio\n"
      ],
      "metadata": {
        "id": "sFZAGiwtAW2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "def process_image(image):\n",
        "    # Step 1: Caption\n",
        "    caption = blip_caption(image)\n",
        "    if is_repetitive(caption):\n",
        "        caption = blip_caption(image, prompt=\"Describe the main object in this image\")\n",
        "    if is_repetitive(caption) or len(caption.split()) < 3:\n",
        "        caption = git_caption(image)\n",
        "\n",
        "    # Step 2: Classify\n",
        "    category = classify_caption(caption)\n",
        "\n",
        "    # Step 3: Log\n",
        "    with open(\"vlm_results_ui.txt\", \"a\") as f:\n",
        "        f.write(f\"Uploaded Image | {caption} | {category}\\n\")\n",
        "\n",
        "    return caption, category\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Textbox(label=\"Caption\"), gr.Textbox(label=\"Category\")],\n",
        "    title=\"VLM Playground\",\n",
        "    description=\"Upload an image to get a caption and category using BLIP + fallback GIT-VLM.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "cxwsxdN_AHvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_with_agents(image):\n",
        "    \"\"\"\n",
        "    Returns: caption (str), category (str), agent_trace (str)\n",
        "    agent_trace: a short history of which agent ran and why (BLIP/GIT, retries, classifier)\n",
        "    \"\"\"\n",
        "    agent_steps = []\n",
        "\n",
        "    # Agent 1: Caption Agent (BLIP)\n",
        "    agent_steps.append(\"CaptionAgent: BLIP (initial)\")\n",
        "    caption = blip_caption(image)\n",
        "    if is_repetitive(caption) or len(caption.split()) < 3:\n",
        "        agent_steps.append(\"CaptionAgent: BLIP retry with stronger prompt\")\n",
        "        caption = blip_caption(image, prompt=\"Describe the main object in this image\")\n",
        "\n",
        "    # If still poor, fallback to GIT\n",
        "    if is_repetitive(caption) or len(caption.split()) < 3:\n",
        "        agent_steps.append(\"CaptionAgent: Fallback -> GIT\")\n",
        "        try:\n",
        "            caption = git_caption(image)\n",
        "        except Exception as e:\n",
        "            agent_steps.append(f\"CaptionAgent: GIT failed ({str(e)})\")\n",
        "            caption = \"Caption generation failed\"\n",
        "\n",
        "    # Agent 2: Classifier Agent\n",
        "    category = classify_caption(caption)\n",
        "    agent_steps.append(f\"ClassifierAgent: matched -> {category}\")\n",
        "\n",
        "    # Agent 3: Logger Agent\n",
        "    log_line = f\"Uploaded Image | {caption} | {category} | Trace: {' > '.join(agent_steps)}\"\n",
        "    with open(\"vlm_results_ui.txt\", \"a\") as f:\n",
        "        f.write(log_line + \"\\n\")\n",
        "    agent_steps.append(\"LoggerAgent: logged to vlm_results_ui.txt\")\n",
        "\n",
        "    agent_trace = \" | \".join(agent_steps)\n",
        "    return caption, category, agent_trace\n"
      ],
      "metadata": {
        "id": "hG14_-dRBCdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=process_image_with_agents,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[gr.Textbox(label=\"Caption\"), gr.Textbox(label=\"Category\"), gr.Textbox(label=\"Agent Trace\")],\n",
        "    title=\"VLM Playground (Agentic Trace)\",\n",
        "    description=\"Upload an image to get a caption, category, and agent trace (BLIP/GIT fallback).\"\n",
        ")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "KYlQ-EY1BMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gradio pandas\n"
      ],
      "metadata": {
        "id": "1cLUjggOBkZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tempfile, os, zipfile\n",
        "\n",
        "# -------- Batch Processing Function --------\n",
        "def process_zip(zip_file):\n",
        "    # Create temp folder\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "    # Extract uploaded ZIP\n",
        "    with zipfile.ZipFile(zip_file.name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "\n",
        "    # Find images recursively\n",
        "    image_paths = glob.glob(f\"{temp_dir}/**/*.*\", recursive=True)\n",
        "    results = []\n",
        "\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            image = Image.open(path).convert(\"RGB\")\n",
        "        except:\n",
        "            continue  # skip non-images\n",
        "\n",
        "        # Step 1: Caption\n",
        "        caption = blip_caption(image)\n",
        "        if is_repetitive(caption):\n",
        "            caption = blip_caption(image, prompt=\"Describe the main object in this image\")\n",
        "        if is_repetitive(caption) or len(caption.split()) < 3:\n",
        "            caption = git_caption(image)\n",
        "\n",
        "        # Step 2: Classify\n",
        "        category = classify_caption(caption)\n",
        "\n",
        "        # Step 3: Agent trace\n",
        "        trace = []\n",
        "        trace.append(\"BLIP\")\n",
        "        if is_repetitive(caption):\n",
        "            trace.append(\"BLIP retry\")\n",
        "        if is_repetitive(caption) or len(caption.split()) < 3:\n",
        "            trace.append(\"GIT fallback\")\n",
        "        trace.append(f\"Classified: {category}\")\n",
        "\n",
        "        results.append({\n",
        "            \"Image Path\": os.path.basename(path),\n",
        "            \"Caption\": caption,\n",
        "            \"Category\": category,\n",
        "            \"Agent Trace\": \" > \".join(trace)\n",
        "        })\n",
        "\n",
        "    # Create DataFrame & save as CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(temp_dir, \"batch_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    return csv_path\n",
        "\n",
        "# -------- Gradio UI --------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üì∑ VLM Playground ‚Äî Single & Batch Mode\")\n",
        "    gr.Markdown(\"Upload a single image **or** a ZIP of images to get captions, categories, and agent traces.\")\n",
        "\n",
        "    with gr.Tab(\"Single Image Mode\"):\n",
        "        single_img = gr.Image(type=\"pil\", label=\"Upload an Image\")\n",
        "        caption_out = gr.Textbox(label=\"Caption\")\n",
        "        category_out = gr.Textbox(label=\"Category\")\n",
        "        trace_out = gr.Textbox(label=\"Agent Trace\")\n",
        "        btn_single = gr.Button(\"Process Image\")\n",
        "        btn_single.click(process_image_with_agents, inputs=single_img, outputs=[caption_out, category_out, trace_out])\n",
        "\n",
        "    with gr.Tab(\"Batch Mode\"):\n",
        "        zip_in = gr.File(label=\"Upload ZIP of Images\", type=\"filepath\")\n",
        "        csv_out = gr.File(label=\"Download Results CSV\")\n",
        "        btn_batch = gr.Button(\"Process Batch\")\n",
        "        btn_batch.click(process_zip, inputs=zip_in, outputs=csv_out)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "nfjqstkGBmrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}